---
title: 大文件分片
date: 2024-10-24T01:22:46+08:00
updated: 2024-10-24T02:06:54+08:00
dg-publish: false
---

`index.js`

```js
const inpFile = document.getElementById('inpFile')

const CHUNK_SIZE = 1024 * 1024 * 5 // 5MB
const THREAD_COUNT = (navigator.hardwareConcurrency - 2) || 4
function cutFile(file) {
  return new Promise((resolve) => {
    const result = []
    let finishedCount = 0
    const chunkCount = Math.ceil(file.size / CHUNK_SIZE)
    const workerChunkCount = Math.ceil(chunkCount / THREAD_COUNT)
    for (let i = 0; i < THREAD_COUNT; i++) {
      const worker = new Worker('./worker.js', {
        type: 'module',
      })
      const startIndex = i * workerChunkCount
      let endIndex = startIndex + workerChunkCount
      if (endIndex > chunkCount) {
        endIndex = chunkCount
      }
      worker.postMessage({
        file,
        CHUNK_SIZE,
        startIndex,
        endIndex,
      })
      worker.onmessage = (e) => {
        for (let i = startIndex; i < endIndex; i++) {
          result[i] = e.data[i - startIndex]
        }
        worker.terminate()
        finishedCount++
        if (finishedCount === THREAD_COUNT) {
          resolve(result)
        }
      }
    }
  })
}

inpFile.addEventListener('change', async (e) => {
  const file = e.target.files[0]
  console.time('cutFile')
  const chunks = await cutFile(file)
  console.timeEnd('cutFile')
  console.log(chunks)
})
```

`worker.js`

```js
import createChunk from './createChunk.js'

onmessage = async (e) => {
  const proms = []
  const { file, CHUNK_SIZE, startIndex, endIndex } = e.data
  for (let i = startIndex; i < endIndex; i++) {
    proms.push(createChunk(file, i, CHUNK_SIZE))
  }
  console.log(proms)
  const chunks = await Promise.all(proms)
  console.log(chunks)
  postMessage(chunks)
}
```

`createChunk.js`

```js
import SparkMD5 from 'spark-md5'

/**
 * 创建文件块，依赖了 spark-md5.js
 * @param {File} file 文件
 * @param {number} start 开始位置
 * @param {number} size 大小
 * @returns {Promise<ArrayBuffer>}
 */
function createChunk(file, index, chunkSize) {
  return new Promise((resolve) => {
    const start = index * chunkSize
    const end = start + chunkSize
    const spark = new SparkMD5.ArrayBuffer()
    const fileReader = new FileReader()
    fileReader.onload = (e) => {
      spark.append(e.target.result)
      resolve({
        start,
        end,
        index,
        hash: spark.end(),
      })
    }
    fileReader.readAsArrayBuffer(file.slice(start, end))
  })
}

export default createChunk
```

依赖了 md5 的库

```js
pnpm add spark-md5
```
